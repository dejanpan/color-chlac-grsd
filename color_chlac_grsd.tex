\documentclass[conference]{sty/IEEEtran}
\usepackage{times}
\usepackage{wrapfig}
\usepackage{tweaklist}
\usepackage{xspace}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{tabularx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{url}
\usepackage{bm}

\def\cH{\mathcal{H}}
\def\cL{\mathcal{L}}
\def\cP{\mathcal{P}}
\def\cO{\mathcal{O}}
\def\mP{{\mathsf P}}
\def\mh{{\mathsf h}}
\def\mo{{\mathsf o}}
\def\mv{{\mathsf v}}
\def\mn{{\mathsf n}}
\def\vcb{{\boldsymbol{c}}}
\def\vpb{{\boldsymbol{p}}}
\def\vqb{{\boldsymbol{q}}}
\def\vnb{{\boldsymbol{n}}}
\def\vlb{{\boldsymbol{l}}}
\def\mr{{\mathsf r}}


% numbers option provides compact numerical references in the text. 
\usepackage[numbers]{natbib}
\usepackage{multicol}
\usepackage[bookmarks=true]{hyperref}

\pdfinfo{
   /Author (Homer Simpson)
   /Title  (Robots: Our new overlords)
   /CreationDate (D:20101201120000)
   /Subject (Robots)
   /Keywords (Robots;Overlords)
}

\begin{document}

% paper title
%\title{Classification of Objects of Daily Use Using Combined Color CHLAC and Global Radius-based Surface Descriptors}
\title{Multidimensional Descriptor for Classification of Objects of Daily Use}

% You will get a Paper-ID when submitting a pdf file to the conference system
\author{Author Names Omitted for Anonymous Review. Paper-ID [add your ID here]}

%\author{\authorblockN{Michael Shell}
%\authorblockA{School of Electrical and\\Computer Engineering\\
%Georgia Institute of Technology\\
%Atlanta, Georgia 30332--0250\\
%Email: mshell@ece.gatech.edu}
%\and
%\authorblockN{Homer Simpson}
%\authorblockA{Twentieth Century Fox\\
%Springfield, USA\\
%Email: homer@thesimpsons.com}
%\and
%\authorblockN{James Kirk\\ and Montgomery Scott}
%\authorblockA{Starfleet Academy\\
%San Francisco, California 96678-2391\\
%Telephone: (800) 555--1212\\
%Fax: (888) 555--1212}}


% avoiding spaces at the end of the author lines is not a problem with
% conference papers because we don't use \thanks or \IEEEmembership


% for over three affiliations, or if they all won't fit within the width
% of the page, use this alternative format:
% 
%\author{\authorblockN{Michael Shell\authorrefmark{1},
%Homer Simpson\authorrefmark{2},
%James Kirk\authorrefmark{3}, 
%Montgomery Scott\authorrefmark{3} and
%Eldon Tyrell\authorrefmark{4}}
%\authorblockA{\authorrefmark{1}School of Electrical and Computer Engineering\\
%Georgia Institute of Technology,
%Atlanta, Georgia 30332--0250\\ Email: mshell@ece.gatech.edu}
%\authorblockA{\authorrefmark{2}Twentieth Century Fox, Springfield, USA\\
%Email: homer@thesimpsons.com}
%\authorblockA{\authorrefmark{3}Starfleet Academy, San Francisco, California 96678-2391\\
%Telephone: (800) 555--1212, Fax: (888) 555--1212}
%\authorblockA{\authorrefmark{4}Tyrell Inc., 123 Replicant Street, Los Angeles, California 90210--4321}}


\maketitle

\begin{abstract}
The abstract goes here.

\end{abstract}

\IEEEpeerreviewmaketitle

\section{Introduction}
One of the great challenges faced by autonomous mobile robot
manipulation is the scaling of the technology to realistic tasks, in
realistic environments under realistic conditions. This challenge
implies that an autonomous household robot is required to act on many
different objects of daily use, to handle them in typical situations
for example when opening a fridge to get the milk or when performing
realistic tasks such as setting the table or preparing a meal.

\subsection{What}
Problem statement:
\begin{itemize}
\item Recogniton and localization of objects of daily use in households
\item Categorization and classification with \emph{one} descriptor
\item Omits a supporting-table assumption and performs in cluttered and occluded scenes
\item Runs online
\end{itemize}


\subsection{Why}
\begin{itemize}
\item to equipp the service robots with the boostrapped recognition models and capabilities
to acquire additional models
\item to enable manipulation tasks in realistic environments under realistic conditions
\item to exploit new sensing technology (e.g. Kinect) and to thus avoid decoupling of gemetrical and
visual appearance data
\end{itemize}


\subsection{How}
\begin{itemize}
\item Combination of view-variant ColorCHLAC and GRSD with the
normalization of the latter by number 27 (number of transitions)
\item Combination of view-invariant version of ColorCHLAC and GRSD
\item Comparisson of Linear Subspace Method and SVM Classifier
\end{itemize}

\subsection{Novelty}


\section{Related Work}
\begin{itemize}
\item VFH~\cite{vfh}
\item GRSD (Humanoids10GRSD~)\cite{GRSD10Humanoids}
\item Color CHLAC (Asako)~\cite{kanezaki2010icra}\cite{kanezaki2010tvc}
\item NARF{}
\item textured-spin images
\item SIFT
\item intensity-spin images Svetlana Lazebnik, Cordelia Schmid, and Jean Ponce. 
    A sparse texture representation using local affine regions. 
    In IEEE Transactions on Pattern Analysis and Machine Intelligence, volume 27, pages 1265-1278, August 2005.
\item sift-keypoint \url{http://www.ros.org/doc/api/pcl/html/sift__keypoint_8h_source.html}
\item Combining Depth and Color Cues for Scale and Viewpoint-Invariant
Object Segmentation and Recognition Using Random Forests - the emphasize is on classifier Random Forests
\end{itemize}

\section{System Overview}


\section{Feature Estimation}

\subsection{Color CHLAC}
Color-CHLAC features are computed from color 3D voxel data by measuring 
    the autocorrelation function of the 3D target object at specific points, 
    represented by local patterns.
Local descriptors are represented by correlation values of the colors of neighboring voxels.

Letting $\bm{x}=(x,y,z)^T$ be the position of a voxel, 
    we use the notation $p(\bm{x})=1$ if the voxel is occupied, and $p(\bm{x})=0$ otherwise. 
When $p(\bm{x})=1$, the voxel has RGB color values.
We represent them as $r(\bm{x})$, $g(\bm{x})$ and $b(\bm{x})$, 
    which are normalized between 0 and 1.
By defining $r'(\bm{x}) \equiv 1 - r(\bm{x})$, $g'(\bm{x}) \equiv 1 - g(\bm{x})$ and $b'(\bm{x}) \equiv 1 - b(\bm{x})$, 
    a voxel status $\bm{f}(\bm{x})\in R^6$ is defined as follows:
%\vspace{-2mm}
\begin{eqnarray*}
  \label{eq:voxel}
%  \hspace{-1mm}
  \bm{f}({\bm x})\hspace{-1mm}=\hspace{-1mm}\left\{
  \begin{array}{cc}
    \hspace{-1mm}
    (r({\bm x})\hspace{1mm} r'({\bm x}) \hspace{1mm}g({\bm x}) \hspace{1mm}g'({\bm x}) \hspace{1mm}b({\bm x}) \hspace{1mm}b'({\bm x}))^T & \hspace{-2mm}(p({\bm x})\hspace{-1mm}=\hspace{-1mm}1) \\
    (0\hspace{1mm}0\hspace{1mm}0\hspace{1mm}0\hspace{1mm}0\hspace{1mm}0)^T & \hspace{-2mm}(p({\bm x})\hspace{-1mm}=\hspace{-1mm}0)
  \end{array}\right.
\end{eqnarray*}
%
Color-CHLAC features are the integral of $\bm{f}(\bm{x})$ or correlations of $\bm{f}(\bm{x})$ between neighboring voxels.
They are calculated by following equations:  
\begin{equation}\label{eq:0th}
  {\bm q} = \int \bm{f}({\bm x}) d{\bm x} 
\end{equation}
%\vspace{-4mm}
\begin{equation}\label{eq:1st}
  {\bm q}({\bm a}) = \int \bm{f}({\bm x}) \hspace{1mm}\bm{f}^T({\bm x}+{\bm a}) d{\bm x} \\
\end{equation}
%
The dimension of Color-CHLAC features calculated by (\ref{eq:0th}) is 6. 
14 patterns are used for the displacement vectors ${\bm a}$ in (\ref{eq:1st}) (Fig. \ref{fig:displacement_vectors}). 
Note that not only $\bm{f}(\bm{x})$ correlation between two neighboring voxels 
    but also the correlation between two elements of $\bm{f}(\bm{x})$ of one voxel is integrated. 

\begin{figure}[htb!]
  \begin{center}
    \includegraphics[width=0.43\textwidth]{figures/colorCHLAC/displacement_vectors.png}
    %\vspace{-2mm}
    \caption{Patterns of displacement vectors. Position of the center voxel is ${\bm x}$, 
      while position of the other highlighted voxel is ${\bm x}+{\bm a}$. }
    \label{fig:displacement_vectors}
  \end{center}
\end{figure}

As a pre-processing of features extraction, $r(\bm{x})$, $g(\bm{x})$ and $b(\bm{x})$ can be binarized. 
Excluding redundant elements, the dimension of Color-CHLAC features calculated by (\ref{eq:1st}) is 
    480 if color values are binarized, and 489 otherwise.
In this paper, we extract Color-CHLAC features both from binarized color voxel data and from original color voxel data. 
Then the dimension of Color-CHLAC feature vector becomes 981 (=6+480+6+489). 


\subsection{GRSD}
In order to perform surface classification, we label the surface units
needed for the global classification  directly, instead
of taking the dominant point-based label for each voxel. This reduces the
complexity proportionally to the average number of points in a voxel. Also,
the neighborhood can be directly constructed using the points in the current
and surrounding voxels.

Once the neighborhood is constructed, we compute the RSD features, i.e. the
radius of the highest and lowest curvature in the local neighborhood, as
described in \cite{Marton10IROS}. As a short overview, from the distribution
of normal angles by distance we take the minimum and maximum normal
variations by distance, and solve the equation:
\begin{align}
\label{eqn:distance_by_angle}
  d_{(\alpha)} = \sqrt{2} r \sqrt{1 - cos(\alpha)}
\end{align}

We can assume $d = r \alpha$ as $\alpha \in [0,\pi/2]$ and the Taylor
decomposition of equation \ref{eqn:distance_by_angle} is:
\begin{align}
\label{eqn:taylor}
  d_{(\alpha)} = r \alpha + \frac{r \alpha^3}{24} + O(\alpha^5)
\end{align}
which greatly reduces the problem of finding $r_{min}$ and $r_{max}$.

Since these values have physical meaning, we can categorize surfaces using
simple, intuitive rules, into: planes (large $r_{min}$), cylinders (medium $r_{min}$,
large $r_{max}$), edges (small $r_{min}$ and $r_{max}$), rims (small $r_{min}$,
medium to large $r_{max}$), and spheres (similar $r_{min}$ and $r_{max}$).

Once all voxels are annotated locally using a geometric class, our
processing pipeline constructs a global feature space that can
produce a unique signature for each object cluster.  This space is
based on the idea that, for a set of labeled voxels, a
global feature can be constructed by observing the relationships between all
these local labels (and the encapsulated free space).
Since the labels represent geometric classes obtained
from the classification of RSD descriptors, we call this new feature the
Global Radius-based Surface Descriptor (GRSD).
The computation of GRSD is similar to GFPFH \cite{Rusu09ICCV-WS}, with the
exception that we sum up the individual $\cH_{f_{ij}}$ histograms instead of
computing their distribution, to further reduce computational complexity. This way,
the complete processing of a cluster (correcting, estimating normals,
computing the voxelized RSD values, labeling voxels and constructing the
GRSD) takes between 0.3 and 0.7 seconds (depending on object size) on a
single core (using voxel size of $1.5cm$ as in the presented examples).


\begin{figure}[htb!]
  \begin{center}
    \includegraphics[width=.4\columnwidth]{figures/grsd/book.png}
\hfill
    \includegraphics[width=.48\columnwidth]{figures/grsd/book_global.png} \\
\hfill
    \includegraphics[width=.3\columnwidth]{figures/grsd/mug.png}
\hfill
    \includegraphics[width=.48\columnwidth]{figures/grsd/mug_global.png}
\caption{Example of RSD classes and GRSD plots for a big flat box (i.e. book, upper row) and a short cylinder (i.e. mug, bottom row).
The histogram bin values are scaled between -1 and 1 according to the
training data, and the colors represent the following local surfaces:
red - sharp edge (or noise), yellow - plane, green - cylinder, light blue -
sphere (not present), and dark blue - rim (i.e. boundary, transition between surfaces).
\emph{Best viewed in color.}
}
    \label{fig:gfpfh}
  \end{center}
\vspace{-2ex}
\end{figure}

\section{Classification Methods}

\subsection{Linear Subspace Method}
\label{sec:subspace}
\subsubsection{Creating subspaces of objects in a database}
First, the voxel data of each object in a database is subdivided into a voxel grid of a certain size, 
    e.g. $10 \times 10 \times 10$ voxels. 
Suppose the $i$-th object in the database is divided into $M_i$ subdivisions.
Then $M_i$ Color-CHLAC feature vectors are extracted. 
To achieve robustness to rotation, 
    features extraction is repeated with various poses of the object. 
The feature vector of an object which is rotated by 90 degrees can be obtained rapidly
    through a simple exchange of the elements of the feature vector in the initial posture.
This is possible because each displacement vector in Fig.~\ref{fig:displacement_vectors}
    is equivalent to another, rotated by 90 degrees.
In this paper, we use this 90 degrees rotation in 24 ways,
    and rotations of 30 and 60 degrees in 21 ways, 
    resulting in achieving 504($=24 \times 21$) varieties of postures for an object. 
Therefore, the number of Color-CHLAC feature vectors 
    generated from each object is $N_i \equiv 504M_i$. 

We represent a Color-CHLAC feature vector compressed by PCA as $\bm{z}_t \in R^d, t=1,2,...N_i$, 
    where $d$ is the dimension of a compressed feature vector.
The auto-correlation matrix of these feature vectors is calculated by the following equation: 
\begin{eqnarray*}
  %\hspace{-1mm}
  R_i = \frac{1}{N_i} \sum^{N_i}_{t=1} \bm{z}_t \bm{z}_t^T
\end{eqnarray*}
The eigenvectors of $R_i$ are then computed by solving the eigenvector problem. 
Finally, the bases of the subspace of the $i$-th object in the database, $P_i \equiv (\bm{v}_{i1} \bm{v}_{i2} ... \bm{v}_{ir})$, 
    are obtained as the $r$ eigenvectors of largest eigenvalue. 

\subsubsection{Calculation of Similarity}
The first step in the recognition process is to compute 
    one Color-CHLAC feature vector from the whole of a query part in a 3D scene. % ???
Then the compressed feature vector $\bm{z}$ is computed using the projection matrix which is 
    generated from all Color-CHLAC feature vectors of each database object in the pre-processing phase.
Let the similarity between the query part and the $i$-th object in the database be $y_i$. 
$y_i$ is defined as the cosine of the angle between $\bm{z}$ and the subspace of the $i$-th object.
$y_i$ is calculated by the following equation: 
\begin{eqnarray}\label{eq:y_calc}
  %\hspace{-1mm}
  y_i = \frac{\| P_i^T \bm{z} \|}{\| \bm{z} \|}
\end{eqnarray}


\subsection{Support Vector Machine-based Classification}


\section{Results}

\subsection{Data Acquisition and Training}
\begin{figure}[htb!]
  \begin{center}
    \includegraphics[width=.45\columnwidth]{figures/rot_table/barilla.png}
\hfill
    \includegraphics[width=.45\columnwidth]{figures/rot_table/barilla1.png} \\
    \includegraphics[width=.45\columnwidth]{figures/rot_table/barilla2.png}
\caption{Acquisition of Training Data}
    \label{fig:data_acquisition}
  \end{center}
\end{figure}
\subsection{Online Testing/Object Recognition}

\begin{figure}[htb!]
  \begin{center}
    \includegraphics[width=.45\columnwidth]{figures/colorCHLAC/detection7.png}
\hfill
    \includegraphics[width=.45\columnwidth]{figures/colorCHLAC/detection5.png} \\
\hfill
    \includegraphics[width=.9\columnwidth]{figures/colorCHLAC/detection2.png}
\caption{An example of the detection of tetrahedral package of milk}
    \label{fig:milk_testing}
  \end{center}
\end{figure}

\begin{figure}[htb!]
  \begin{center}
    \includegraphics[width=.9\columnwidth]{figures/colorCHLAC/GRSD_CCHLAC.png}
    \caption{Examples of GRSD-ColorCHLAC histograms.}
    \label{fig:grsd_cchlac}
  \end{center}
\end{figure}

\section{Conclusions and Future Work} 
The conclusion goes here.

\section*{Acknowledgments}
CoTeSys
%% Use plainnat to work nicely with natbib. 

\bibliographystyle{plainnat}
\bibliography{references}

\end{document}


%TODO:
%Check the original template and see what the meant with the hyperlinks
