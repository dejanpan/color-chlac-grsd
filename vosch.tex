% \documentclass[conference]{sty/IEEEtran}
\documentclass[letterpaper, 10 pt, conference]{sty/ieeeconf}
\IEEEoverridecommandlockouts
\overrideIEEEmargins

\usepackage{cite}
\usepackage{times}
\usepackage{wrapfig}
\usepackage{tweaklist}
\usepackage{xspace}
\usepackage{graphicx}
\usepackage{tabularx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{url}
\usepackage{bm}
\usepackage{color}
\usepackage{colortbl}
\usepackage{subfig}
\usepackage[ruled,vlined]{algorithm2e}

\def\cH{\mathcal{H}}
\def\cL{\mathcal{L}}
\def\cP{\mathcal{P}}
\def\cO{\mathcal{O}}
\def\mP{{\mathsf P}}
\def\mh{{\mathsf h}}
\def\mo{{\mathsf o}}
\def\mv{{\mathsf v}}
\def\mn{{\mathsf n}}
\def\vcb{{\boldsymbol{c}}}
\def\vpb{{\boldsymbol{p}}}
\def\vqb{{\boldsymbol{q}}}
\def\vnb{{\boldsymbol{n}}}
\def\vlb{{\boldsymbol{l}}}
\def\mr{{\mathsf r}}


% numbers option provides compact numerical references in the text.
%\usepackage[numbers]{natbib}
\usepackage{multicol}
\usepackage[bookmarks=true]{hyperref}

% \pdfinfo{
%    /Author (Homer Simpson)
%    /Title  (Robots: Our new overlords)
%    /CreationDate (D:20101201120000)
%    /Subject (Robots)
%    /Keywords (Robots;Overlords)
% }

% paper title
%\title{Classification of Objects of Daily Use Using Combined Color CHLAC and Global Radius-based Surface Descriptors}
%\title{Multidimensional Descriptor for Classification of Objects of Daily Use}
\title{\LARGE \bf Voxelized Shape and Color Histograms for RGB-D}

% You will get a Paper-ID when submitting a pdf file to the conference system
%\author{Paper-ID [110]}

%% \author{\authorblockN{Asako Kanezaki, Tatsuya Harada, Yasuo Kuniyoshi}
%% \authorblockA{
%% Graduate School of Information Science and Technology,\\
%% The University of Tokyo \\ %7-3-1 Hongo Bunkyo-ku, Tokyo Japan \\
%% Email: \{kanezaki, harada, kuniyosh\}@isi.imi.i.u-tokyo.ac.jp}
%% \and
%% \authorblockN{Dejan Pangercic, Zoltan-Csaba Marton, Michael Beetz}
%% \authorblockA{
%% Intelligent Autonomous Systems Group,\\ TU Munich \\
%% Email: \{pangercic, marton, beetz\}@cs.tum.edu}}

\author{Asako Kanezaki, Dejan Pangercic, Zoltan-Csaba Marton, \\ Tatsuya Harada, Yasuo Kuniyoshi, and Michael Beetz% <-this % stops a space
%\thanks{This work was not supported by any organization}% <-this % stops a space
\thanks{A. Kanezaki, T. Harada, and Y. Kuniyoshi are with Graduate School of Information Science and Technology, The University of Tokyo
%, 7-3-1 Hongo Bunkyo-ku, Tokyo Japan
}
\thanks{D. Pangercic, Z.-C. Marton, and M. Michael Beetz are with Intelligent Autonomous Systems Group, TU Munich
%, KarlStr. 45, 80333 Munich Germany
}
\thanks{\tt\small kanezaki@isi.imi.i.u-tokyo.ac.jp}
\thanks{\tt\small harada@isi.imi.i.u-tokyo.ac.jp}
\thanks{\tt\small kuniyosh@isi.imi.i.u-tokyo.ac.jp}
\thanks{\tt\small pangercic@cs.tum.edu}
\thanks{\tt\small marton@cs.tum.edu}
\thanks{\tt\small beetz@cs.tum.edu}
}

\begin{document}

\newcommand{\todo}[1]{\textbf{\textcolor{red}{TODO: #1}}}
\maketitle
\thispagestyle{empty}
\pagestyle{empty}

\begin{abstract}
Real world environments typically include objects with different perceptual appearances.
A household, for example, includes textured, textureless and even partially transparent objects.
While perception methods exist that work well on one such class of objects, the perception of
scenes including different classes of objects is still a challenge.
It is our view that a step towards solving these problems is the
construction of a descriptor that takes both color and shape
into account -- with high discriminating power.
In this paper, we
present an approach to efficiently capture these two dimensions
into a single feature
that is capable of capturing both the geometric and visual appearance
properties of common objects of daily use. We showcase the
feature's applicability for the purpose of classification of objects in cluttered scenes with occlusions,
and we evaluate two classification approaches.
For the experiments we make use of Kinect, a new RGB-D device,
and build a database of 63 objects. Preliminary results
on novel views show recognition rates of 72.2\%.
% recognition rate on novel views is 58% for LSM if computed correctly
\end{abstract}

%\IEEEpeerreviewmaketitle

\section{Introduction}
One of the great challenges in autonomous mobile robot manipulation is scaling
technology to realistic tasks in real-world environments under realistic
conditions. This means that an autonomous household robot must act on many
different objects of daily use, handle them in typical situations, e.g. when
opening a fridge to get milk or when performing challenging everyday tasks
such as setting the table or preparing a meal.

%\todo{Provide the definition of descriptor vs detector vs feature}\\
Descriptor-based object recognition has proved, in recent years, to be very successful.
SIFT~\cite{lowe04distinctive}, SURF~\cite{surf} and others can
detect, localize and recognize many types of textured objects efficiently, reliably
and under varying lighting conditions. Likewise, descriptors have been developed 
for perceiving objects based on their shapes (e.g. circles, cylinders, spheres and hybrid variants).
Normal Aligned Radial Features (NARF)\cite{steder10irosws} for range images, 
and several versions of Point Feature  Histograms (PFH, FPFH)~\cite{Rusu09ICRA} and 
Viewpoint Feature Histograms (VFH)~\cite{vfh} for unordered fully-3D point clouds 
are the most notable ones. While these descriptors were successful, too, they are still limited because they
typically work only in restricted feature spaces. So, in order to have a perception system
based on SIFT features, one needs to restrict the search space to objects that are detected by
texture and if we apply 3D features, the objects must be distinctive with respect to their shapes. 
If we want to install a perception system for autonomous robots we need to aim 
at one that can perceive all objects, be they textured, texture-less, different shapes, etc.

\begin{figure}[tb!]
  \begin{center}
    \includegraphics[width=.99\columnwidth]{figures/firstpage/firstpage.pdf}
    \includegraphics[width=.99\columnwidth]{figures/objects/objects.jpg}
    \caption{Top: Autonomous service robot equipped with a Kinect sensor
    acquiring training data of the objects shown in the bottom image. For 
  every view of the object, a VOSCH or ConVOSCH descriptor was estimated and a database
of 63 objects was generated. An object detection pipeline with Support Vector
Machines or Linear Subspace Method classifiers was then used to detect objects
in natural scenes. Bottom: the objects used in our experiments.}
    \label{fig:robot}
  \end{center}
\end{figure}

While it seems feasible that one can simply develop perception routines by combining
various descriptors~\cite{stueckler10combining, GRSD10Humanoids}, a more promising idea 
is to develop descriptors that work in multiple feature spaces. The reason for this is 
that objects might be easily discriminated in a combined feature space, whereas it is impossible 
to discriminate them in individual spaces as depicted in Figure~\ref{fig:grsd_cchlac}.
Our approach is thus to be seen as an alternative to the bag-of-experts type
of approaches~\cite{Varma07learningthe}.

In this paper, we report on our work on the two variations of a novel descriptor
which builds upon the idea of the
Global Radius-based Surface Descriptor (GRSD~\cite{GRSD10Humanoids}) and the
Circular Color Cubic Higher-order Local Auto-Correlation descriptor (C$^3$-HLAC~\cite{kanezaki2011icra}).
We termed it (Concatenated) Voxelized Shape and Color Histograms -- (Con)VOSCH.

% \subsection{What}
% In this paper, we address the problem of localization and recognition of
% objects of daily use in households. To this effect, we employ a multidimensional
% feature descriptor that captures both the geometrical and visual appearance
% properties of the objects.

The strength of the descriptor lies in its ability to consider geometric and
visual features in a unified manner to facilitate object classification. The two
underlying features (GRSD, C$^3$-HLAC) have been carefully selected because of 
their similar structures and the way they are computed from voxelized 3D data. 
While ConVOSCH is a mere concatenation of GRSD and C$^3$-HLAC histograms, 
VOSCH is developed such that it allows for its computation in a single step because 
of the modified underlying algorithmic properties for GRSD and C$^3$-HLAC. 
Both features count relations between neighboring voxels, capturing geometric 
surface type and texture transitions at the same time.
These features can be used to perform classification of one object against 63 
different objects (see Figure~\ref{fig:robot}) in 0.1 millisecond using Linear
Subspace Method Classifier~\cite{watanabe1973}.

Further contributions of the work presented herein include the ability to deal with
cluttered scenes while eliminating the requirement for region of interest
selection (e.g. horizontal supporting structures) and spatially separable object
clusters. This has been achieved by ensuring that the descriptor is additive and by using a
moving bounding box-based approach together with a sub-space matching-based classification scheme (Section
\ref{sec:recognition}). 
%% The resulting approach performs at 2 frames per second
%% while taking into account an object database of 63 different objects
%% (see Figure~\ref{fig:robot}).


% \subsection{Why}
% In the past, strong features have been proposed in perception for personal robots,
% both working on 2D and 3D data. Prominent examples include
% SIFT~\cite{lowe04distinctive} and SURF~\cite{surf} for camera images, Normal Aligned Radial
% Features (NARF)\cite{steder10irosws} for range images, and several versions of Point Feature
% Histograms (PFH, FPFH)~\cite{Rusu09ICRA} and Viewpoint Feature Histograms
% (VFH)~\cite{vfh} for unordered fully-3D point clouds.
% \todo{Nico proof-read till here}


% Yet it seems that there is none that can capture
% all important and obvious properties (such as geometry and color are)
Lastly, our descriptor can be applied out-of-the-box to the data coming from 
new sensing technologies such as the Kinect device and lays the foundation stone for a
multitude of applications \todo{remove ``such as'' and fix reference?} such as the detection of a priori learned object models discussed
further on (Section~\ref{sec:recognition}).
% % % VOSCH also enables equipping of service robots 
% % % (such as the robot shown in top part of Figure~\ref{fig:robot}) with the 
% % % capabilities to acquire additional object models which furthermore opens ways 
% % % for manipulation tasks in realistic environments under realistic conditions.

\begin{figure}[tb!]
  \begin{center}
    \includegraphics[width=.99\columnwidth]{figures/colorCHLAC/artificial/normalized_hist.jpg}
    %% \includegraphics[width=.48\columnwidth]{figures/colorCHLAC/artificial/purple.png}
    %% \includegraphics[width=.48\columnwidth]{figures/colorCHLAC/artificial/torus.png}
%    \includegraphics[width=.48\columnwidth]{figures/colorCHLAC/artificial/purple.png}
%    \includegraphics[width=.48\columnwidth]{figures/colorCHLAC/artificial/torus.png}
    \caption{Examples of scaled VOSCH histograms.
Left: different categories of objects (top-down: cylinder, cube, cone, plane, sphere, torus, die) 
have different values in the first 20 bins of their histograms.
Right: different colors of the same category of a torus have different values in the last 117 bins of their histograms.}
    \label{fig:grsd_cchlac}
  \end{center}
\end{figure}

The remainder of the paper is structured as follows. After the overview of the related
work in Section~\ref{sec:rl}, we present a system overview in Section~\ref{sec:overview}
which is followed by Section~\ref{sec:features}, which elaborates on the construction of the
ConVOSCH and VOSCH descriptor variants. In Section~\ref{sec:classification}, we present the usage of this descriptor
for the classification of objects using two classification methods which we
then test and present in Section~\ref{sec:results}. In Section~\ref{sec:conclusion},
we conclude and give an outlook on our future work.

\section{Related Work}
\label{sec:rl}
%\item Color CHLAC (Asako)~\cite{kanezaki2011icra}\cite{kanezaki2010tvc}
% \item textured-spin images~\cite{Johnson_spin_images}
% \item A sparse texture representation using local affine
% regions~\cite{Lazebnik05asparse}
% \item sift-keypoint
% \url{http://www.ros.org/doc/api/pcl/html/sift__keypoint_8h_source.html}
Expressiveness and efficiency of the descriptor are both crucially important for
a real-time object recognition system.  Though there is a trade-off between
these two abilities, selecting a
proper matching method which accounts for the characteristics of the
descriptor will improve both expressiveness and efficiency. 
In the recent decade, a corpus of powerful 2D and 3D descriptors for
recognizing real objects have been developed. However, few of them take into
account the compatibility with the matching method used with them.

The SIFT~\cite{lowe04distinctive} descriptor, one of the most well-known 2D
descriptors for object recognition, makes use of detected keypoints in
scenes and compares them with those of reference objects to identify the
objects currently observed. A combination of the visual appearance
descriptor and 2D shape is presented by Bosch et al. in~\cite{Bosch07shape}
where they use random forests to achieve around 80\% classification 
accuracy. 

The NARF~\cite{steder10irosws} descriptor detects salient points in range images of
real environments. Despite its dominant
repeatability in point-to-point matching, there still remain difficulties in
cluster-to-cluster matching, which is necessary for identifying each cluster
candidate as an object in the environment. This also causes high computational cost
especially when the target object database is large. In a 3D domain, a recently 
developed VFH~\cite{vfh} descriptor was presented as an extension 
of~\cite{Rusu09ICRA}. The feature's discriminative power is increased
by the inclusion of the viewpoint, which, on the other hand, also represents
a deficiency in that the feature becomes orientation variant. 

There are some approaches that use both geometry and color descriptors to
represent 3D bodies~\cite{park2006}. However, properly balancing these two
different properties is difficult.  Caputo and Dorko~\cite{caputo2002} learned
respective kernels for shape and color representations and combined them for
object recognition, while Huang and Hilton~\cite{huang2009} balanced them by
normalizing bins of shape-color histograms.  An alternative solution for combining
geometry and color information in proper balance is to extract descriptors
represented by patterns of shape-and-color co-occurrence.  For example, textured
spin-image~\cite{cortelazzo2006} splits well-known
spin-image~\cite{Johnson_spin_images} descriptors into several layers
according to given levels of luminance. C$^3$-HLAC~\cite{kanezaki2011icra} 
splits the CHLAC~\cite{kobayashi2004} descriptor, which is a histogram of 14 patterns 
depending on the relative position of neighboring voxels, into RGB channels and the correlation 
space of these channels.  In \cite{kanezaki2011icra}, the C$^3$-HLAC descriptor is used as a local
descriptor in the training process and as a global descriptor describing each
object cluster in the recognition process. This takes advantage of the combination
of the Linear Subspace Method (LSM)~\cite{watanabe1973} and the descriptor's additive property, 
which means a global descriptor for an object cluster is computed by summing up the local descriptors of its sub-parts.
Since computation of C$^3$-HLAC is on voxelized data, it is more efficient
than the point-based approaches. It was shown by the authors in \cite{kanezaki2011icra} to outperform textured spin-images.

\section{System Overview}
\label{sec:overview}
The general idea of the work presented herein is depicted in 
Figure~\ref{fig:robot}. We first acquire the training data for 63 objects
of daily use using the RGBD Kinect sensor. We then estimate ConVOSCH and VOSCH descriptors 
for every object view and generate training models using Support Vector
Machines~\cite{svm99} (SVM) or Linear Subspace Method~\cite{watanabe1973} (LSM) classifiers. The models are then used
in the evaluation part for cross-validation checks and the detection of 
objects in natural scenes.

% % % Since the focus of the paper is on three parts, i.e. i)VOSCH descriptor
% % % construction, ii)database assembly and training of object models and iii)testing,
% % % we solved each of them individually.
To construct the ConVOSCH and VOSCH descriptors, we had to modify the original algorithms
such that the C$^3$-HLAC became rotation invariant and GRSD 
additive. We thus obtained descriptor histograms with 
\emph{1001} bins for ConVOSCH and \emph{137} bins for VOSCH representing the transitions between its different dimensions.
For constructing a database of object models (Figure~\ref{fig:robot} bottom)
containing training examples, we used a rotating
table using a pan-tilt unit that is controlled by the robot over the
network. Objects placed on this rotating table were scanned at a given
angle-step ($15^\circ$) and then used to classify objects found in
typical household settings.
% % % For this third case we also performed a test on objects that were
% % % segmented in the euclidean sense. 

Our approach is realized as part of the Robot Operating System
(ROS, \url{http://www.ros.org}) open source framework, and makes
use of modular and robust components that can be reused on other robotic
systems different from ours.

% \subsection{Performance and Novelty}
An important factor of the perception system for robotics is to perform fast. We thus optimized
the proposed descriptors to bring the estimation time down to $0.26$ s for a cluster consisting of 
\emph{4632} points on average, and to have the classification time of under \emph{50} milliseconds
for the SVM classifier and 0.1 millisecond for LSM. The Moving-bounding-box-based
recognition of objects in e.g. cluttered scenes takes 2-4 seconds as detailed
in Section~\ref{sec:recognition}.

The main contributions of this paper are the following:
\begin{itemize}
\item specification of a novel descriptor with two variants (ConVOSCH and VOSCH) that account for
geometrical as well as visual appearance  properties of objects;
\item comparison of classification results for two established classification
frameworks -- LSM and SVM;
\item an extensive database of 63 objects of daily use captured with the Kinect sensor
used as training examples (which we intend to publish after the review process).
\end{itemize}

\section{Feature Estimation}
\label{sec:features}
As already mentioned earlier, the construction of ConVOSCH and VOSCH features is based on the idea of 
C$^3$-HLAC and GRSD features. For the sake of clarity we will thus briefly present
their respective construction.
Next, we will present the necessary modifications to them in order to obtain the proposed features.
% Since both approaches work on voxelized data, making their computation a single process
% and combining them in a single feature is an efficient solution.

\subsection{C$^3$-HLAC}
\label{sec:color_chlac}
C$^3$-HLAC descriptor is a high-dimensional vector measuring the summation of the 
multiplied RGB values of neighboring voxels in a $3 \times 3 \times 3$ grid around a voxel grid of arbitrary size. 
Each bin in a C$^3$-HLAC descriptor is differentiated by the RGB color space and the 
relative position of the two neighboring voxels. 
Let $\bm{x}=(x,y,z)^T$ be the position of a voxel, $p(\bm{x})$ be the flag for occupancy 
of the voxel, and $r(\bm{x})$, $g(\bm{x})$ and $b(\bm{x})$ be its RGB values normalized 
between 0 and 1, respectively.  By defining 
$r_1(\bm{x}) \equiv sin\left( \frac{\pi}{2}r(\bm{x})\right)$, $g_1(\bm{x}) \equiv sin\left( \frac{\pi}{2}g(\bm{x})\right)$, $b_1(\bm{x}) \equiv sin\left( \frac{\pi}{2}b(\bm{x})\right)$, 
$r_2(\bm{x}) \equiv cos\left( \frac{\pi}{2}r(\bm{x})\right)$, $g_2(\bm{x}) \equiv cos\left( \frac{\pi}{2}g(\bm{x})\right)$, and $b_2(\bm{x}) \equiv cos\left( \frac{\pi}{2}b(\bm{x})\right)$, 
a voxel status $\bm{f}(\bm{x})\in \mathbb{N}^6$ is defined as follows: 
\begin{eqnarray*}
%  \hspace{-1mm}
  \bm{f}({\bm x})\hspace{-1mm}=\hspace{-1mm}\left\{
  \begin{array}{cc}
    \hspace{-1mm}
    \left[r_1({\bm x})\hspace{1.5mm} r_2(\bm{x}) \hspace{1.5mm}g_1({\bm x}) \hspace{1.5mm}g_2({\bm x}) \hspace{1.5mm}b_1({\bm x}) \hspace{1.5mm}b_2({\bm x})\right]^T & \hspace{-2mm}p({\bm x})\hspace{-1mm}=\hspace{-1mm}1 \\
    \left[\hspace{2mm} 0\hspace{1mm}0\hspace{1mm}0\hspace{1mm}0\hspace{1mm}0\hspace{1mm}0 \hspace{2mm} \right]^T & \hspace{-2mm}p({\bm x})\hspace{-1mm}=\hspace{-1mm}0
  \end{array}\right.
\end{eqnarray*}
%
%Color-CHLAC features are the integral of $\bm{f}(\bm{x})$ or correlations of $\bm{f}(\bm{x})$ between neighboring voxels.
Letting ${\bm a_i}$ be a displacement vector from the reference voxel to its neighboring voxel, 
the elements of a C$^3$-HLAC descriptor extracted from a voxel grid $V$ are calculated by the following equations:
\begin{equation}\label{eq:0th}
  {\bm q_1} = \sum_{{\bm x} \in V} \bm{f}({\bm x})
\end{equation}
%\vspace{-2mm}
\begin{equation}\label{eq:0th_1}
  {\bm q_2} = \sum_{{\bm x} \in V} \bm{f}({\bm x}) \hspace{1mm}\bm{f}^T({\bm x}) \\
\end{equation}
%\vspace{-2mm}
\begin{equation}\label{eq:1st}
  {\bm q_3}({\bm a}_i) = \sum_{{\bm x} \in V} \bm{f}({\bm x}) \hspace{1mm}\bm{f}^T({\bm x}+{\bm a_i}) \;\;\; (i=0, \dots 12) \\
\end{equation}
%
Since these values are summed up around a voxel grid, there is redundant computation of the 
same value over symmetric pairs of ${\bm a_i}$. 
As a result, the number of variations in ${\bm a_i}$ is 13, which is a half of all the 26 neighbors in a $3\times3\times3$ grid.
The matrix computed by (\ref{eq:1st}) is expanded into a column vector of 36 elements.
Therefore the dimension of the vector calculated by (\ref{eq:0th}) is 6, while that by 
(\ref{eq:1st}) is 468 (=$36\cdot13$).
The second part of the C$^3$-HLAC descriptor is computed from the binarized values of $r(\bm{x})$, $g(\bm{x})$ 
and $b(\bm{x})$. 
To decide the threshold of color binarization, 
    we apply the histogram threshold selection method of~\cite{otsu1979} to the R, G and B values respectively, 
    using the voxel colors of all objects in the database as sample data.
C$^3$-HLAC features calculated by (\ref{eq:0th_1}) include redundant elements, e.g., $r(\bm{x})\cdot g(\bm{x})$ and $g(\bm{x})\cdot r(\bm{x})$.
Excluding the redundant elements, the dimension is thus 12 if color values are binarized, and 21 otherwise (see Figure~\ref{fig:eq3_colorCHLAC}). 
Finally a full C$^3$-HLAC vector is obtained by concatenating the two vectors from binarized 
color voxel data and from original color voxel data. Then the dimension of C$^3$-HLAC feature vector 
becomes 981 (6+468+12 for non-binarized data plus 6+468+21 for binarized data). 

\begin{figure}[tb!]
  \begin{center}
    \includegraphics[width=.99\columnwidth]{figures/colorCHLAC/Eq3_in_colorCHLAC.png}
  \caption{Elements in C$^3$-HLAC features calculated by (\ref{eq:0th_1}). $r_1$ and $r_2$ represents $r_1(\bm{x})$ and $r_2(\bm{x})$, respectively. White grids are excluded since they are redundant.}
  \label{fig:eq3_colorCHLAC}
\end{center}
\end{figure}

\subsection{GRSD}
\label{sec:grsd}
GRSD is a histogram, that counts the number of transitions between different types of voxels.
While it is applicable to the wide range of applications we used it to count the transitions between 
the following geometric classes of voxel surfaces: \emph{free space, plane, cylinder, sphere, rim, and noise}.
The type of the surfaces are identified based on the estimation of their
two principal curves' radii, $r_{min}$ and $r_{max}$ \cite{Marton10IROS}. 
This approach is using the leaves of an octree as voxels, to facilitate fast ray intersection
tests and is summarized in Algorithm~\ref{alg:compute_grsd}.
\begin{algorithm}[htb!]
L = \{$l_{1}...l_{m}$\} \tcc{\footnotesize{octree leaves encapsulating the object}}
C = \{$c_{l_1}...c_{l_m}$\} \tcc{\footnotesize{the leaf classes generated by $RSD$ 
    (e.g. class empty, class plane, etc.)}}
$\cH$ = ZeroMatrix \tcc{\footnotesize{transition matrix holding the number of neighboring classes}}
  \ForEach{$l_{i} \in L$}
  {
    \ForEach{$l_{j} \in L$}
    {
      $r_{ij} \leftarrow (l_{i},l_{j})$ \tcc{\footnotesize{create a line segment between $l_{i}$ and $l_{j}$}}
      $I = \{l_{k}|l_{k} = L \cap r_{ij}\}$ \tcc{\footnotesize{get leaves intersected by $r_{ij}$}}
      \tcc{\footnotesize{count the class changes between leaves in $I$}}
      \ForEach{$l_{k} \in I$}
      {
        $\cH[c_{l_k},c_{l_{k+1}}]++$ \tcc{\footnotesize{increase counter}}
      }
    }
  }
  \caption{GRSD Computation}
  \label{alg:compute_grsd}
\end{algorithm}
The number of bins $b$ is:
\begin{equation}
b=\frac{s(s+1)}{2}
\end{equation}
where $s$ is the number of possible surface types,
resulting in 21 dimensions for the above stated 6 types of surfaces.

In order to efficiently merge the two features, the original GRSD had to be altered (see next subsection). 
In the new, additive GRSD$_2$ feature, transitions from \emph{free space} to \emph{free space} are impossible,
hence its dimensionality becomes 20.

\subsection{VOSCH}
\label{sec:VOSCH}
In order to generate VOSCH we refined C$^3$-HLAC descriptor to be rotation-invariant by bringing all  
13 different vectors given by (\ref{eq:1st}) together by the following equation: 
\begin{equation}\label{eq:1st_new}
  {\bm q_4} = \sum_{i=0}^{12} {\bm q_3}({\bm a}_i) = \sum_{i=0}^{12}\sum_{{\bm x} \in V} \bm{f}({\bm x}) \hspace{1mm}\bm{f}^T({\bm x}+{\bm a_i}) \\
\end{equation}
%
This reduces the dimensionality of the descriptor down to 117 (6+36+12 for non-binarized data plus 6+36+21 
for binarized data) with respect to the original implementation. 
% This is the similar way that GRSD takes as described in the next subsection, where the transitions between the reference 
% voxel and all the 26 nighbors are combined together. 
By doing so the refined C$^3$-HLAC descriptor is well matched with GRSD which gives a significant 
robustness to rotation and still shows a powerful expressiveness for color textures.

GRSD, as shown in~\cite{GRSD10Humanoids}, originally used
ray-tracing in order to find transitions between surface types. To make it fit into VOSCH 
we i)~omitted ray-tracing in favor of fast-to-compute adjacency voxels checks and ii)~discarded
the normalization of histogram bins.

These modifications allow us to create histograms which, like in the case of C$^3$-HLAC, are additive.
The latter property can best be described as follows. 
If we break up an object into several parts, 
the object's histogram becomes equal to the sum of the histograms of its sub-parts. 
%
%% If we break up objects into reasonable sizes, 
%% its histogram can be approximated by the sum of the histograms of its sub-cells. 
%% %Unlike in C$^3$-HLAC, free space is considered, but we are looking
%% %at the neighbors of occupied voxels only. 
%% \todo{The additive feature is approximate, as for the voxels at the boundary of one part 
%% only the neighbors from the same part are considered. This is not critical unless 
%% one considers extremely few voxels separately. The difference gets less and less 
%% as the size of the parts increase.}
%
 Furthermore, since we do not use ray-casting but 
consider only the direct neighbors of each occupied cell, the computation of the new GRSD$_2$ is in 
the millisecond range. This is 2 orders of magnitude faster than the original implementation,
% reported in~\cite{GRSD10Humanoids}.
at the cost of some descriptiveness.

\subsection{ConVOSCH}
The idea of ConVOSCH is to simply estimate C$^3$-HLAC and GRSD$_2$ in two separate steps, 
concatenate resulting histograms and normalize the final result into a range of [0,1]. This preserves the 
high dimensionality and thus accuracy for the color space, but on the other hand 
makes the feature to the extent rotation-variant and slightly slower for use 
in the classification schemes.
\vspace{2ex}

The advantage of the VOSCH and ConVOSCH features is depicted in Figure~\ref{fig:comparison},
where we see that the visual appearance-based feature such as C$^3$-HLAC can not
discern the orange cube from the orange die (note that the histograms in bottom-left part are identical). 
On the other hand we have a shape-based feature GRSD/GRSD$_2$
which can not discern between the orange and the blue die (likewise the
histograms in bottom-right part identical too).

\begin{figure}[tb!]
  \begin{center}
    \includegraphics[width=.3\columnwidth]{figures/comparison/cube.png}
    \includegraphics[width=.32\columnwidth]{figures/comparison/dice1.png}
    \includegraphics[width=.32\columnwidth]{figures/comparison/dice2.png}
    \includegraphics[width=.47\columnwidth]{figures/comparison/orange_cube_vs_orange_dice.png}
    \includegraphics[width=.47\columnwidth]{figures/comparison/blue_vs_orange_dice_grsd.png}
  \caption{Rotation-invariant C$^3$-HLAC can not differentiate the die from the cube (identical histograms on the left), 
    while GRSD/GRSD$_2$ can not differentiate the different colors (identical histograms on the right). 
    Their combination however produces distinct signatures for all of them (Figure~\ref{fig:grsd_cchlac}).}
  \label{fig:comparison}
\end{center}
\end{figure}

\section{Classification Methods}
\label{sec:classification}

\subsection{Linear Subspace Method}
\label{sec:subspace}
LSM is an established learning method for classification. 
For the first step, we solve PCA for all the descriptors extracted from all the trained objects, 
and then use the top $d$ dimensions of the descriptor as a feature vector, in the same way as \cite{kanezaki2011icra}.
As a training process, several feature vectors that represent each object are extracted, 
and then Principal Component Analysis (PCA) for them is solved. 
Similarly to ~\cite{kanezaki2011icra}, we divide the whole voxel grid of each object into 
cubic subdivisions of a certain size ($7$~cm~$\times~7$~cm~$\times~7$~cm with $5$~cm~$\times~5$~cm~$\times~5$~cm overlapping in our case), and then extract feature vectors from all the subdivisions for training. 
Let number of the subdivisions be $N$ 
and the training feature vectors be $\bm{z}_t \in R^d, t=1,2,...N$. 
Then the eigenvectors of $R=\frac{1}{N} \sum^{N}_{t=1} \bm{z}_t \bm{z}_t^T$ are computed. 

To classify and identify a detected object in environments, the system extracts one feature vector from the whole voxel grid of the object. 
Letting the feature vector be $\bm{z}$ and the matrix of top $c$ eigen vectors for the $i$-th object in the database be $P_i \equiv (\bm{v}_{i1} \bm{v}_{i2} ... \bm{v}_{it})$,
the similarity between the query part and the $i$-th object in the database is calculated by the following equation:
\begin{eqnarray}\label{eq:y_calc}
  %\hspace{-1mm}
  y_i = \frac{\| P_i^T \bm{z} \|}{\| \bm{z} \|}
\end{eqnarray}
There is an advantage of using LSM with this kind of histogram-based descriptors which have the additive property. 
The additive property means that a global descriptor for an object cluster equals the summation of local descriptors of its sub-parts. 
Owing to this property, one feature vector computed from each object cluster in environments can be classified by projecting it to 
each subspace of database objects, regardless of the size of the object cluster, thus providing a fairly fast method for classifying partially visible objects.

\subsection{Support Vector Machine-based Classification}
We also performed classification using SVM in our experiments.
SVM is a fast classification method that works by learning the vectors that define hyperplanes separating
the training examples according to a cost and a kernel function.

Unlike LSM, it is not well suited for recognizing partially visible objects using the presented features,
unless partial views are explicitly trained. Another problem is over-fitting to the training data, which
can be limited by choosing the parameters such that they maximize the results of cross-validation checks
on the training data.

We used a C-SVC type SVM with a radial basis function kernel \cite{LIBSVM} with $\gamma = 0.0078125$
($0.5$ for GRSD$_2$). Cost values higher than $128$ did not improve the final results further.

\subsection{Scaling of Features}
To improve classification rates using LSM it is important not to have values of the feature 
varying over several orders of magnitude with respect to others and thus becoming very important components
during PCA. Similarly, for SVM the results are also improved if the variation of each feature bin is
proportional. Therefore we performed bin-wise scaling of the features for classification as follows.
When going through the training data, the minimum and maximum value of each bin is recorded, and this interval 
is mapped to the interval [0,1] both during training and during testing.


\section{Results}
\label{sec:results}
To evaluate the proposed features we ran
tests using SVM and LSM classifiers on the following two sets of data: 
i)~7 artificially generated objects with 7 different colors, and
ii)~63 objects of daily use, scanned using a Kinect sensor and a rotating table
(shown in the bottom part of Figure~\ref{fig:robot}). 
We measured the recognition rates against the original implementations of C$^3$-HLAC and 
GRSD. \todo{was this GRSD or GRSD$_2$? update text and tables/graphs if necessary}
Since C$^3$-HLAC was already shown to outperform Textured Spin Images~\cite{cortelazzo2006}
we did not include it in our experiments.

\subsection{Feature Extraction}
\label{sec:feature_extraction}
In all tests we parametrized the computation of the feature with the following parameters:
\begin{itemize}
\item search radius for normals: $2$~cm
\item search radius for $r_{min}$ and $r_{max}$ estimation: $1$~cm
\item voxel size: $1$~cm
\end{itemize}

The execution times for VOSCH feature estimation are shown in the following table, where 
$\overline{nr_{points}}$ denotes the average number of points per object,
$\overline{t_{point}}$ is the average estimation time per point,
and $\overline{t_{object}}$ is the average estimation time needed for an object:
\newcommand{\mc}[3]{\multicolumn{#1}{#2}{#3}}
\definecolor{tcA}{rgb}{0.784314,0.784314,0.784314}
\begin{table}[tb]
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
\rowcolor{tcA} & \hspace{-0.5mm}$\overline{nr_{points}}$\hspace{-0.5mm} & \hspace{-0.5mm}$\overline{t_{point}}$\hspace{-0.5mm} & \hspace{-0.5mm}$\overline{t_{object}}$\hspace{-0.5mm} \\
\hline
\mc{1}{|>{\columncolor{tcA}}c|}{\hspace{-1mm}synthetic data (49 views of 49 objects)\hspace{-1mm}} & 19124 & 36 $\mu$s & 0.69 s \\
\hline
\mc{1}{|>{\columncolor{tcA}}c|}{real data (1512 views of 63 objects)} & 4632 & 57 $\mu$s & 0.26 s \\
\hline
\end{tabular}
\caption{Times needed for the estimation of VOSCH.}
\label{tbl:timing}
\end{center}
\end{table}

\subsection{Synthetic Data}
We first carried out tests on a set of 49 artificially generated objects, consisting of
7 shapes, each in 7 colors (Figure~\ref{fig:grsd_cchlac}). We used 1 example of 
each object as a training sample and evaluated the model on 5 examples of each object with 
10 different noise levels. The noise levels were generated for points' 3D coordinates 
using Gaussian distribution with the following standard deviations:
\begin{center}
$\sigma \in \{0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0\}$ [mm]
\end{center}
The deviations were set to correspond to the actual noise levels of a Kinect 
sensor\footnote{\url{http://www.ros.org/wiki/openni_kinect/kinect_accuracy}}.
For LSM classification we set the dimension of the compressed feature space $d$ to 
50 for C$^3$-HLAC, ConVOSCH and VOSCH, while GRSD was left uncompressed. 
The dimensionality of the subspace $c$ was set to 3.
The results of the test in Figure~\ref{fig:synthetic},
show VOSCH (right-most two columns) outperforming all other features.
%% For a better overview of the results using VOSCH please also see Figure~\ref{fig:plot}.

Interrestingly, LSM is slightly better than SVM at low noise levels, but is superseded
by SVM as noise increases. This is because SVM avoids over-fitting by maximizing the margin
between the different classes, while LSM penalizes deviations from the model
(i.e. ``regression line'' of the training features).

%% \begin{figure}[tb!]
%%   \begin{center}
%%     \includegraphics[width=.98\columnwidth]{figures/comparison/vosch.pdf}
%%     \caption{The effect of simulated noise on the classification results using VOSCH with LSM and SVM
%%              (both with and without random rotations of test data, as presented in Figure~\ref{fig:synthetic}).}
%%     \label{fig:plot}
%%   \end{center}
%% \end{figure}

Since GRSD is insensitive to color, it can identify at most $1/7 \approx 14.2857\%$ of the data.
Additionally, since objects were uniformly colored and mostly symmetrical, the fact that C$^3$-HLAC 
is not rotation invariant did not influence the results in the table. In the original implementation C$^3$-HLAC 
had to be trained with the artificial rotations to achieve rotation invariance.

%% \begin{table*}[tb]
%% \begin{center}
%% \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}\hline
%% % use packages: color,colortbl
%% \rowcolor{tcA} \textbf{} & \textbf{} & \mc{2}{>{\columncolor{tcA}}c|}{\textbf{GRSD / 20D}} & \mc{2}{>{\columncolor{tcA}}c|}{\textbf{C$^3$-HLAC / 981D}} & \mc{2}{>{\columncolor{tcA}}c|}{\textbf{ConVOSCH / 1001D}} & \mc{2}{>{\columncolor{tcA}}c|}{\textbf{VOSCH / 137D}}\\
%% \cline{3-10}
%% \rowcolor{tcA} \textbf{Noise STD} & \textbf{Rotation} & \textbf{LSM} & \textbf{SVM} & \textbf{LSM} & \textbf{SVM} & \textbf{LSM} & \textbf{SVM} & \textbf{LSM} & \textbf{SVM}\\
%% \hline
%% \mc{1}{|>{\columncolor{tcA}}c|}{\textbf{0.5}} & \mc{1}{>{\columncolor{tcA}}c|}{\textbf{None:}} & 14.2857\% & 14.2857\% & 100\% & 100\% & 85.7142\% & 100\% & 100\% & 100\% \\
%% \rowcolor{tcA} \textbf{mm} & \textbf{Random:} & 14.2857\% & 12.6531\% & 35.1020\% & 70.2041\% & 83.2653\% & 79.5918\% & 98.3673\% & 91.4286\% \\
%% \hline
%% \mc{1}{|>{\columncolor{tcA}}c|}{\textbf{1.0}} & \mc{1}{>{\columncolor{tcA}}c|}{\textbf{None:}} & 14.2857\% & 14.2857\% & 71.4285\% & 77.1429\% & 85.7142\% & 85.7143\% & 100\% & 100\% \\
%% \rowcolor{tcA} \textbf{mm} & \textbf{Random:} & 14.2857\% & 14.2857\% & 34.6938\% & 65.7143\% & 82.8571\% & 78.7755\% & 100\% & 99.5918\% \\
%% \hline
%% \mc{1}{|>{\columncolor{tcA}}c|}{\textbf{1.5}} & \mc{1}{>{\columncolor{tcA}}c|}{\textbf{None:}} & 14.2857\% & 13.0612\% & 54.2857\% & 28.5714\% & 85.7142\% & 85.7143\% & 100\% & 91.4286\% \\
%% \rowcolor{tcA} \textbf{mm} & \textbf{Random:} & 14.2857\% & 13.0612\% & 20.4081\% & 26.1224\% & 81.6326\% & 82.449\% & 100\% & 90.6122\% \\
%% \hline
%% \mc{1}{|>{\columncolor{tcA}}c|}{\textbf{2.0}} & \mc{1}{>{\columncolor{tcA}}c|}{\textbf{None:}} & 13.8775\% & 12.2449\% & 28.5714\% & 28.5714\% & 73.4693\% & 85.7143\% & 94.2857\% & 85.7143\% \\
%% \rowcolor{tcA} \textbf{mm} & \textbf{Random:} & 13.4693\% & 12.2449\% & 17.1428\% & 20.8163\% & 67.3469\% & 85.7143\% & 95.9183\% & 85.7143\% \\
%% \hline
%% \mc{1}{|>{\columncolor{tcA}}c|}{\textbf{2.5}} & \mc{1}{>{\columncolor{tcA}}c|}{\textbf{None:}} & 12.2448\% & 10.2041\% & 28.5714\% & 28.5714\% & 57.9591\% & 57.1429\% & 85.7142\% & 71.4286\% \\
%% \rowcolor{tcA} \textbf{mm} & \textbf{Random:} & 12.2448\% & 10.2041\% & 16.3265\% & 25.7143\% & 51.4285\% & 57.1429\% & 85.7142\% & 71.4286\% \\
%% \hline
%% \mc{1}{|>{\columncolor{tcA}}c|}{\textbf{3.0}} & \mc{1}{>{\columncolor{tcA}}c|}{\textbf{None:}} & 12.2448\% & 10.2041\% & 28.5714\% & 28.5714\% & 57.1428\% & 57.1429\% & 82.8571\% & 71.4286\% \\
%% \rowcolor{tcA} \textbf{mm} & \textbf{Random:} & 12.2448\% & 10.2041\% & 17.5510\% & 28.5714\% & 53.8775\% & 55.5102\% & 82.8571\% & 71.4286\% \\
%% \hline
%% \mc{1}{|>{\columncolor{tcA}}c|}{\textbf{3.5}} & \mc{1}{>{\columncolor{tcA}}c|}{\textbf{None:}} & 11.0204\% & 10.2041\% & 28.5714\% & 28.5714\% & 57.1428\% & 57.1429\% & 68.5714\% & 71.4286\% \\
%% \rowcolor{tcA} \textbf{mm} & \textbf{Random:} & 11.8367\% & 10.2041\% & 15.5102\% & 28.5714\% & 50.2040\% & 45.3061\% & 71.4285\% & 71.4286\% \\
%% \hline
%% \mc{1}{|>{\columncolor{tcA}}c|}{\textbf{4.0}} & \mc{1}{>{\columncolor{tcA}}c|}{\textbf{None:}} & 8.9795\% & 8.57143\% & 25.7142\% & 28.5714\% & 49.7959\% & 43.2653\% & 57.1428\% & 68.5714\% \\
%% \rowcolor{tcA} \textbf{mm} & \textbf{Random} &  9.7959\% & 8.57143\% & 15.1020\% & 28.5714\% & 39.5918\% & 42.8571\% & 66.5306\% & 70.2041\% \\
%% \hline
%% \mc{1}{|>{\columncolor{tcA}}c|}{\textbf{4.5}} & \mc{1}{>{\columncolor{tcA}}c|}{\textbf{None:}} & 7.3469\% & 8.16327\% & 14.2857\% & 28.5714\% & 42.8571\% & 42.8571\% & 54.2857\% & 74.2857\% \\
%% \rowcolor{tcA} \textbf{mm} & \textbf{Random:} & 7.7551\% & 7.7551\% & 15.1020\% & 28.5714\% & 34.2857\% & 44.898\% & 57.1428\% & 68.1633\% \\
%% \hline
%% \mc{1}{|>{\columncolor{tcA}}c|}{\textbf{5.0}} & \mc{1}{>{\columncolor{tcA}}c|}{\textbf{None:}} & 8.1632\% & 5.71429\% & 14.2857\% & 28.5714\% & 42.4489\% & 53.4694\% & 57.1428\% & 53.8776\% \\
%% \rowcolor{tcA} \textbf{mm} & \textbf{Random:} & 8.5714\% & 6.12245\% & 15.5102\% & 28.5714\% & 32.6530\% & 52.6531\% & 55.1020\% & 52.2449\% \\
%% \hline
%% \end{tabular}
%% \caption{Comparison of the features and classifiers on the artificial data. \emph{None} corresponds to the test data being in the same pose as the artificial training data, and \emph{Random} denotes randomly rotated test data.}
%% \label{tbl:synthetic}
%% \end{center}
%% \end{table*}

\begin{figure*}[tb!]
  \centering
  \includegraphics[width=.49\textwidth]{figures/synthetic_experiment/synthetic_experimental_result_1.png}
  \includegraphics[width=.49\textwidth]{figures/synthetic_experiment/synthetic_experimental_result_2.png}
  \caption{The effect of simulated noise on the classification results using GRSD, C$^3$-HLAC, ConVOSCH and VOSCH with LSM and SVM
    (both with and without random rotations of test data).}
  \label{fig:synthetic}
\end{figure*}


\subsection{Real Data}
\label{sec:real_data}
\subsubsection{Acquisition of Training Data}
Our database of 3D objects was obtained using the Kinect sensor mounted on the head of the robot.
The set of objects (see Figure~\ref{fig:robot}) encompasses the ones commonly used
in a typical household environment (mugs, utensils, groceries, etc) and is envisioned for a
larger expansion and a release after the review process. We rotated the objects on the rotating table with
an angular step of $15^\circ$ around the up-axis, and acquired partial
snapshots from a perspective that best approximates the robot's view point during its working cycle.  
% We consider this to be an important point as opposed to similar initiatives (e.g.,
% \cite{kit}) where the datasets are acquired using high-precision and off-board sensors.

% \begin{figure}[tb!]
%   \begin{center}
%     \includegraphics[width=.32\columnwidth]{figures/rot_table/barilla.png}
%     \includegraphics[width=.32\columnwidth]{figures/rot_table/barilla1.png}
%     \includegraphics[width=.32\columnwidth]{figures/rot_table/barilla2.png}
%     \caption{Acquisition of training data using a rotating table controlled by the robot.}
%     \label{fig:data_acquisition}
%   \end{center}
% \end{figure}

% \begin{figure}[tb!]
%   \begin{center}
%     \includegraphics[width=.9\columnwidth]{figures/colorCHLAC/real/tomato/tomato_hist_pcd.png}
%     \includegraphics[width=.9\columnwidth]{figures/colorCHLAC/real/tomato/tomato_hist_pcd2.png}
%     \caption{Examples of \todo{scaled} VOSCH histograms for a tomato soup can.}
%     \label{fig:grsd_colorchlac_tomato}
%   \end{center}
% \end{figure}


\subsubsection{Evaluation on Training Data}
Table~\ref{tbl:training} shows the percentage of correctly classified training examples using the model computed for them.
The numbers in parentheses are the percentages of correctly classified views using the model constructed using the remaining views
(leave-one-out-test).
For LSM classification we set $d$ to 
100 for C$^3$-HLAC, ConVOSCH and VOSCH, while GRSD was left uncompressed. 
The dimensionality of the subspace $c$ was set to 10 for GRSD and 50 for the others.

% \begin{table*}[tb]
% \begin{center}
% \begin{tabular}{|c|c|c|c|c|}
% \hline
% \rowcolor{tcA} & \textbf{GRSD (20D)} & \textbf{Color-CHLAC (981D)} & \textbf{Concatenated (ConVOSCH)} & \textbf{VOSCH (137D)} \\
% \hline
% \mc{1}{|>{\columncolor{tcA}}c|}{\textbf{LSM}} & 29.6958\% (17.2619\%) & 99.0741\% (97.8175\%) & 99.3386\% (96.8915\%) & 97.8836\% (93.1217\%) \\
% \hline
% \mc{1}{|>{\columncolor{tcA}}c|}{\textbf{SVM}} & 99.2063\% (66.9974\%) & 99.8677\% (99.6693\%)  & 99.9339\% (99.6032\%) & 100\% (99.1402\%) \\
% \hline
% \end{tabular}
% \caption{Model Accuracy on Training Data (and Leave-One-Out Test) for VOSCH \todo{less decimals and 1 column}}
% \label{tbl:training}
% \end{center}
% \end{table*}

\begin{table}[tb]
\begin{footnotesize}
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
\rowcolor{tcA} & \textbf{GRSD} & \textbf{C$^3$-HLAC} & \textbf{ConVOSCH} & \textbf{VOSCH} \\
\rowcolor{tcA} & \textbf{20D [\%]} & \textbf{981D [\%]} & \textbf{1001D [\%]} & \textbf{137D [\%]} \\
\hline
\mc{1}{|>{\columncolor{tcA}}c|}{\textbf{LSM}} & 43.65 (39.48) & 99.67 (99.6) & \textbf{99.8} (\textbf{99.74}) & \textbf{99.8} (99.67) \\
\hline
\mc{1}{|>{\columncolor{tcA}}c|}{\textbf{SVM}} & 99.07 (73.81) & \textbf{99.87} (99.6)  & \textbf{99.87} (\textbf{99.67}) & 99.8 (98.94) \\
\hline
\end{tabular}
\caption{Model accuracy on training data and for the leave-one-out test -- the latter in parentheses.}
\label{tbl:training}
\end{center}
\end{footnotesize}
\end{table}
% GRSD:     32.0 0.5 73.8095
% C3HLAC:   
% ConVOSCH: 
% VOSCH:    

\subsubsection{Evaluation on Novel Views}
To evaluate the proposed features on novel views as well, we performed the test with the
following setup (shown in Figure~\ref{fig:novel_views}): we selected 5 types of scenes containing textured
and textureless objects (a and b), objects with similar shapes (c), a scene with the substantial 
altered lighting conditions (d), and objects with arbitrary rotation (e). We acquired the data from 3 substantially different views 
as also shown in Figure~\ref{fig:novel_views}. Object candidate clusters were detected
by first finding a major horizontal planar surface within the point cloud, as done in~\cite{Rusu09IROS_ClosingLoop}.
For these experiments, we tested all the choices with a break for ten (10, 20, \dots) as the dimension of subspace $c$ in LSM, and set the best one.

In total we tested on 72 views, achieving the highest rates with SVM, as shown in Table~\ref{tbl:novel}.
%The results for VOSCH were slightly less convincing, but more testing is needed to make a final judgment.
One way to try to improve the results is to treat each view as a separate sub-class, which shall  increase
the actual classification results by avoiding the training of very different views of the same object into one class.

\begin{table}[tb]
\begin{scriptsize}
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
\rowcolor{tcA} & \textbf{} & \textbf{GRSD} & \textbf{C$^3$-HLAC} & \textbf{ConVOSCH} & \textbf{VOSCH} \\
\rowcolor{tcA} & \textbf{} & \textbf{20D} & \textbf{981D} & \textbf{1001D} & \textbf{137D} \\
\hline
\mc{1}{|>{\columncolor{tcA}}c|}{\textbf{(a)}} & \mc{1}{>{\columncolor{tcA}}c|}{\textbf{LSM}} & 16.7\% & 75\% & \textbf{91.7\%} & 83.3\% \\
\mc{1}{|>{\columncolor{tcA}}c|}{\textbf{texture}} & \mc{1}{>{\columncolor{tcA}}c|}{{SVM}} & 50\% & 75\% & \textbf{83.3\%} & 66.7\% \\
\hline
\mc{1}{|>{\columncolor{tcA}}c|}{\textbf{(b)}} & \mc{1}{>{\columncolor{tcA}}c|}{\textbf{LSM}} & 16.7\% & 44.4\% & \textbf{61.1\%} & 44.4\% \\
\mc{1}{|>{\columncolor{tcA}}c|}{\textbf{no texture}} & \mc{1}{>{\columncolor{tcA}}c|}{{SVM}} & 44.4\% & \textbf{66.7\%} & 61.1\% & 61.1\% \\
\hline
\mc{1}{|>{\columncolor{tcA}}c|}{\textbf{(c)}} & \mc{1}{>{\columncolor{tcA}}c|}{\textbf{LSM}} & 5.6\% & 33.3\% & 44.4\% & \textbf{55.6\%} \\
\mc{1}{|>{\columncolor{tcA}}c|}{\textbf{sim. shape}} & \mc{1}{>{\columncolor{tcA}}c|}{\textbf{SVM}} & 22.2\% & 61.1\% & 72.2\% & \textbf{77.8\%} \\
\hline
\mc{1}{|>{\columncolor{tcA}}c|}{\textbf{(d)}} & \mc{1}{>{\columncolor{tcA}}c|}{\textbf{LSM}} & 5.6\% & 50\% & 50\% & \textbf{61.1\%} \\
\mc{1}{|>{\columncolor{tcA}}c|}{\textbf{diff. light}} & \mc{1}{>{\columncolor{tcA}}c|}{\textbf{SVM}} & 22.2\% & \textbf{88.9\%} & \textbf{88.9\%} & 72.2\% \\
\hline
\mc{1}{|>{\columncolor{tcA}}c|}{\textbf{(e)}} & \mc{1}{>{\columncolor{tcA}}c|}{\textbf{LSM}} & 16.7\% & 16.7\% & 16.7\% & \textbf{100\%} \\
\mc{1}{|>{\columncolor{tcA}}c|}{\textbf{arb. rotation}} & \mc{1}{>{\columncolor{tcA}}c|}{\textbf{SVM}} & 0\% & 16.7\% & 33.3\% & \textbf{50\%} \\
% \hline
% \mc{1}{|>{\columncolor{tcA}}c|}{\textbf{(e)}} & \mc{1}{>{\columncolor{tcA}}c|}{\textbf{LSM}} & 0\% & 50\% & 50\% & 66.6\% \\
% \mc{1}{|>{\columncolor{tcA}}c|}{\textbf{free view}} & \mc{1}{>{\columncolor{tcA}}c|}{\textbf{SVM}} &  &  &  &  \\
% \hline
\hline
\hline
\mc{1}{|>{\columncolor{tcA}}c|}{\textbf{Total}} & \mc{1}{>{\columncolor{tcA}}c|}{\textbf{LSM}} & 11.1\%$^*$ & 45.8\% & 55.6\% & \textbf{63.9\%} \\
\mc{1}{|>{\columncolor{tcA}}c|}{\textbf{ }} & \mc{1}{>{\columncolor{tcA}}c|}{\textbf{SVM}} & 30.6\%$^*$ & 68.1\% & \textbf{72.2\%} & 68.1\% \\
%\mc{2}{|>{\columncolor{tcA}}c|}{\textbf{Total for SVM}} & \textbf{33.3\%$^*$} & \textbf{72.7\%} & \textbf{75.8\%} & \textbf{69.7\%} \\
\hline
\end{tabular}
\end{center}
\end{scriptsize}
\caption{Model accuracy on novel view data. $^*$Please note that while in the original implementation GRSD was used for detecting the geometric category,
         here we used it for classification of objects that were only visually distinguishable (different brands of milk boxes for example).}
\label{tbl:novel}
\end{table}
% GRSD(10) C3HLAC(20) ConVOSCH(40) VOSCH(70)

While LSM proved to be robust to noise on the synthetic data, the real data is quite smooth,
and SVM outperformed it in our tests. LSM has however the advantage of exploiting the additive
property of the feature for detection in clutter. Moreover, the time required for classification is significantly shorter with LSM than with SVM.

%% \todo{Prove it works for a)textured objects, b)texture-less objects, c)objects with different shapes, 
%% d)varying lighting conditions -- no clutter, 1 object at a time}
%\todo{Dejan}

\subsection{Object Detection Pipeline (in Clutter)}
\label{sec:recognition}
We applied our object recognition method on an object detection scheme in a household
environment. The system computes the similarities between each target object and all 
rectangular-solid sub-regions in the environment, and it then outputs all the regions
that have higher similarities than an empirically set threshold.
Each sub-region has the same size as that of the bounding box of the target object 
($20$~cm~$\times~20$~cm~$\times~20$~cm) in our tests.
To accelerate feature extraction from the sub-regions, we used ``Integral Feature Table''~\cite{kanezaki2010tvc}, 
which is a simple extension of ``Integral Image''~\cite{integral_image} from 2D to 3D.
The ``Integral Feature Table'' $\bm{I}(x,y,z)$ is defined as the $d$-dimensional 
compressed feature vector extracted from the voxel area ranging from $(0,0,0)$ to $(x,y,z)$.
Let the feature vector of the voxel area with $x$ ranging from $x_1$ to $x_2$,
$y$ ranging from $y_1$ to $y_2$, and $z$ ranging from $z_1$ to $z_2$, be $\bm{F}(x_1,y_1,z_1,x_2,y_2,z_2)$.
The latter is then computed by the following equation:
\begin{eqnarray*}\label{eq:sat}
\bm{F}(x_1,y_1,z_1,x_2,y_2,z_2) = \bm{I}(x_2,y_2,z_2) - \bm{I}(x_1,y_2,z_2)
                           \\ - \bm{I}(x_2,y_1,z_2) - \bm{I}(x_2,y_2,z_1)
                           \\ + \bm{I}(x_1,y_1,z_2) + \bm{I}(x_1,y_2,z_1)
                           \\ + \bm{I}(x_2,y_1,z_1) - \bm{I}(x_1,y_1,z_1)
\end{eqnarray*}
Using the ``Integral Feature Table'', $\bm{F}(x_1,y_1,z_1,x_2,y_2,z_2)$ can always be 
computed by adding the 8 cached feature vectors, regardless of the size of the target object.
To furthermore accelerate the process empty regions are skipped as well.
Similarly to the ``Integral Feature Table'', we create a table that stores
the number of voxels with the occupied property, in the area ranging from $(0,0,0)$ to $(x,y,z)$.
Using this table, the number of voxels with the occupied property in the detection 
box can be computed quickly by adding 8 scalar values.

This object detection pipeline was applied on six objects from the set of 
earlier discussed 63 training objects. Objects were placed onto several locations on a kitchen counter and the robot was tasked to actively find them.

Preliminary results of the correctly detected objects are shown in Figure~\ref{fig:correct_detected}. 
When the range of the target point cloud in the environment is limited to $1$~m from the viewpoint, 
all the processing including voxelization, feature extraction and similarity calculation was performed 
in 2-4 seconds per point cloud with the size of 307000 points.

%\todo{Asako: description for the results}

\begin{figure*}[tb!]
  \centering
  \subfloat[Textured objects]{\label{fig:textured}\includegraphics[width=.2\textwidth]{figures/novel_view_scenes/text_view1.png}}
  \subfloat[Textureless objects]{\label{fig:textureless}\includegraphics[width=.2\textwidth]{figures/novel_view_scenes/textureless_view2.png}}
  \subfloat[Two groups of objects with similar shapes]{\label{fig:similar_shapes}\includegraphics[width=.2\textwidth]{figures/novel_view_scenes/similar_shape_view1.png}}
  \subfloat[Altered light conditions]{\label{fig:light}\includegraphics[width=.2\textwidth]{figures/novel_view_scenes/diff_light_view3.png}}
  \subfloat[Arbitrary rotation]{\label{fig:light}\includegraphics[width=.2\textwidth]{figures/novel_view_scenes/rotation.png}}
 \caption{Screenshots of the novel view scenes (best viewed in color) }
 \label{fig:novel_views}
\end{figure*}

\begin{figure*}[tb!]
  \centering
  \includegraphics[width=.99\textwidth]{figures/detection_demo/four_objects_detection.png}
  \caption{Screenshots of the examples of correctly detected objects as marked by cubes in different colors corresponding to the background colors of the target objects shown in the left two columns (best viewed in color). }
  \label{fig:correct_detected}
\end{figure*}



\section{Conclusions and Future Work}
\label{sec:conclusion}

We presented and evaluated a way to efficiently combine two
appearance characteristics into a single feature (both rotation
variant and rotation invariant) and showed its advantage over
the original methods.

The rotation invariant VOSCH feature with its low number of dimensions
promises to be an efficient and descriptive feature that scales well with
the increased number of objects a real-world application of the system would
require. C$^3$-HLAC is not rotation invariant, so it has to be trained with
all rotations around the view-ray, thus ConVOSCH as well. We found
however, that at least for our 63 objects, the texture variations
were not significantly different when the objects were rotated
-- neither for the single colored synthetic data (Figure~\ref{fig:synthetic}),
nor for the real views (Table~\ref{tbl:novel}).

Since the features were designed to be additive, they can be used with LSM for
detection in cluttered scenes by using a sliding box approach.
%% Since the features were designed to be additive, they can be used using
%% LSM and detection in cluttered scenes using a sliding box approach.

Unfortunately, acquiring enough labeled training data with enough variation
to avoid overfitting is a problem in 3D perception, thus the final results are
not completely informative and have high variations.
Furthermore, the dataset is quite challenging, containing many similar objects of same type or brand.
We will try to address these issues by expanding and publishing our database of objects
and creating more labeled scenes for evaluation.

Additionally, the automatic building of object databases by an in-hand modeling by the
robot itself is a useful tool that we plan to investigate in order to allow for
easier upgrade of the database with novel objects.
% requires grasping without recognition - cite IROS10... maybe for the final version

Future work will include incorporation of other promising 2.5/3D approaches like VFH and NARF
into the voxelized feature extraction process, and evaluating using more test and training data.

% % % \todo{color stabilization?}


% \section*{Acknowledgments}
% This work was supported by the DFG cluster of excellence \emph{CoTeSys} (Cognition for Technical Systems). \todo{Willow?}\\
% \todo{Asako: Do we need to ackknowledge someone from ISI Lab?}
% %% Use plainnat to work nicely with natbib.

%\bibliographystyle{plainnat}
\bibliographystyle{./IEEEtran} % use IEEEtran.bst style
\bibliography{references}
\end{document}


%TODO:
%Check the original template and see what the meant with the hyperlinks
