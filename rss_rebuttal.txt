=Rev1=
We would like to thank all reviewers for their constructive and critical 
remarks. We will address all wording and grammar issues as well the difficulties
with comprehending of formulae. In the continuation of this document we are going 
to clarify all misunderstood or wrongly interpreted parts of the paper.
First off we would like to stress that the main novel contribution of the submitted paper 
is in the theoretical consideration and design of the new feature and not 
so much in getting the industrial, bullet-proof recognition system working.
The latter issue is, as clearly stated in the paper, on our future research agenda.
To "3x3x3 grid" - will be fixed. "Handling of binarized colors" - we provide the 
reference #12. "Dimensions 12 and 21 following Eq 3" - [Asako]. "On GRSD being
terse desribed" - we will provide modified GRSD implementation as an algorithm.
"On classification of partially visible objects" - this claim is supported
by reference #12. "On Kinect's noise" an analysis is available here: 
http://www.ros.org/wiki/openni_kinect/kinect_accuracy (note: not authors' site).
"On training from views similar to that obtained by real robot" - this but obvious
that the training and the testing data should be similar. "On results in Table II" -
Of especial interest are columns 4-7 which present classification results for ConVOSCH
and VOSCH for test data with different noise levels and compare them to GRSD and 
ColorCHLAC (columns 1-4). Take-home message is that the former outperform the latter.
"On slightly less convincing VOSCH results" - we will remove this claim as Table IV 
gives actual factual results. "On how to describe shape and color" - the lack
of reliable Bag-Of-Experts system and the success of features for 2D data (e.g. SIFT)
and 3D data (e.g. FPFH) suggest that developing all-in-one is a clear way to go.
=Rev2=
"the real image/real objects experiments are less convincing" the best three features performed similarly,
but VOSCH has only 137 dimensions, while the other two around 1000. Since during testing we used the objects
in the same upright orientation as during testing, the rotation invariance did not play a role. However,
we would include experiments with arbitrarily orientated objects in the final version.
"detection of objects in clutter" was already shown and evaluated using ColorCHLAC, and we use the same approach here.
We agree that more experiments have to be performed and will address this issue and discuss the results.
=Rev3=
We will include timings also for the original features. The use of LSM is in the possibility of applying it without any pre-segmentation step.
Please also see the comments above. Regarding "If we break up objects...", it is approximate, as for the voxels at the boundary
the neighbors that are outside the considered region are not considered. However, this is not critical unless one considers extremely few
voxels separately. This is what "reasonable size" refers to, that for example breaking up an object in cubes consisting of 2x2x2 voxels
and summing the features up, would produce a different feature than the one computed for the whole object. This difference gets less and less
as the size of the parts increase.
=Rev4=
